#!/usr/bin/env python3
"""
éŸ³é¢‘ç”Ÿæˆæµ‹è¯•è„šæœ¬ - åœ¨Stable Audioç¯å¢ƒä¸­
- è¯»å–ç”Ÿæˆçš„promptæ–‡ä»¶
- ç”ŸæˆéŸ³é¢‘å¹¶ä¸Šä¼ åˆ°Supabase
- åˆ›å»ºè‹±æ–‡è¯„ä¼°ç•Œé¢
"""

import os
import sys
import time
import json
import torchaudio
import numpy as np
import webbrowser
import asyncio
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
import torch
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.stable_audio_service import stable_audio_service
from app.services.storage_service import storage_service

class AudioTester:
    """éŸ³é¢‘æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = []
        self.output_dir = Path("audio_test_output")
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "audio_files").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)
        (self.output_dir / "evaluation").mkdir(exist_ok=True)
        
    def load_prompts(self, prompt_file: str = None) -> List[Dict[str, Any]]:
        """åŠ è½½promptæ–‡ä»¶"""
        if prompt_file is None:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„promptæ–‡ä»¶
            prompt_dir = Path("generated_prompts")
            if not prompt_dir.exists():
                print("âŒ æœªæ‰¾åˆ°generated_promptsç›®å½•")
                return []
            
            prompt_files = list(prompt_dir.glob("generated_prompts_*.json"))
            if not prompt_files:
                print("âŒ æœªæ‰¾åˆ°promptæ–‡ä»¶")
                return []
            
            # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
            prompt_file = max(prompt_files, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“ è‡ªåŠ¨é€‰æ‹©promptæ–‡ä»¶: {prompt_file}")
        
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompts = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(prompts)} ä¸ªprompt")
            return prompts
        except Exception as e:
            print(f"âŒ åŠ è½½promptæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def analyze_audio_comprehensive(self, audio_path: str) -> Dict[str, Any]:
        """ç®€åŒ–çš„éŸ³é¢‘åˆ†æ - åªä¿ç•™åŸºç¡€æŒ‡æ ‡"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(audio_path):
                return {"error": f"Audio file not found: {audio_path}"}
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(audio_path)
            if file_size == 0:
                return {"error": f"Audio file is empty: {audio_path}"}
            
            # åŠ è½½éŸ³é¢‘
            waveform, sample_rate = torchaudio.load(audio_path)
            
            # è½¬æ¢ä¸ºfloat32è¿›è¡Œè®¡ç®—
            waveform = waveform.float()
            
            duration = waveform.shape[1] / sample_rate
            channels = waveform.shape[0]
            
            # åŸºç¡€æŒ‡æ ‡
            dynamic_range = torch.max(waveform) - torch.min(waveform)
            rms = torch.sqrt(torch.mean(waveform ** 2))
            silence_ratio = torch.sum(torch.abs(waveform) < 0.01) / waveform.numel()
            
            # é¢‘è°±åˆ†æ
            if channels == 2:  # ç«‹ä½“å£°
                left_channel = waveform[0]
                right_channel = waveform[1]
                stereo_correlation = torch.corrcoef(torch.stack([left_channel, right_channel]))[0, 1]
            else:
                stereo_correlation = 1.0
            
            # å³°å€¼æ£€æµ‹
            peak_ratio = torch.sum(torch.abs(waveform) > 0.9) / waveform.numel()
            
            # é¢‘è°±å¯†åº¦åˆ†æ
            fft = torch.fft.fft(waveform.mean(dim=0))
            spectral_density = torch.abs(fft)
            spectral_centroid = torch.sum(spectral_density * torch.arange(len(spectral_density))) / torch.sum(spectral_density)
            
            # è¿‡é›¶ç‡ (Zero Crossing Rate) - è¡¡é‡éŸ³é¢‘çš„å¤æ‚åº¦
            zero_crossings = torch.sum(torch.sign(waveform[:, 1:]) != torch.sign(waveform[:, :-1]))
            zero_crossing_rate = zero_crossings / (waveform.shape[0] * (waveform.shape[1] - 1))
            
            return {
                "duration": float(duration),
                "channels": int(channels),
                "sample_rate": int(sample_rate),
                "dynamic_range": float(dynamic_range),
                "rms": float(rms),
                "silence_ratio": float(silence_ratio),
                "stereo_correlation": float(stereo_correlation) if isinstance(stereo_correlation, (float, int)) else stereo_correlation.item(),
                "peak_ratio": float(peak_ratio),
                "spectral_centroid": float(spectral_centroid),
                "zero_crossing_rate": float(zero_crossing_rate),
                "file_size": file_size
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    async def run_single_test(self, prompt_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        print(f"\n=== æµ‹è¯•: {prompt_data['id']} ===")
        print(f"æ¨¡å¼: {prompt_data['mode']}")
        print(f"ç”¨æˆ·è¾“å…¥: {prompt_data['user_input']}")
        print(f"æœ€ç»ˆprompt: {prompt_data['final_prompt']}")
        
        start_time = time.time()
        
        try:
            # è®¾ç½®é»˜è®¤æ—¶é•¿
            duration = 8.0
            
            # ç”ŸæˆéŸ³é¢‘
            print(f"ğŸ”„ æ­£åœ¨ç”ŸæˆéŸ³é¢‘...")
            audio_path = stable_audio_service.generate_audio(
                prompt=prompt_data['final_prompt'],
                duration=duration
            )
            
            generation_time = time.time() - start_time
            
            # ä¸Šä¼ åˆ° Supabase
            print(f"â˜ï¸ æ­£åœ¨ä¸Šä¼ åˆ° Supabase...")
            cloud_url = await storage_service.upload_audio(audio_path, prompt_data['final_prompt'])
            
            if cloud_url:
                print(f"âœ… ä¸Šä¼ æˆåŠŸ: {cloud_url}")
            else:
                print("âŒ ä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
                cloud_url = None
            
            # åˆ†æéŸ³é¢‘
            metrics = self.analyze_audio_comprehensive(audio_path)
            
            result = {
                "id": prompt_data['id'],
                "user_input": prompt_data['user_input'],
                "mode": prompt_data['mode'],
                "inspiration_chip": prompt_data['inspiration_chip'],
                "atmosphere": prompt_data['atmosphere'],
                "elements": prompt_data['elements'],
                "final_prompt": prompt_data['final_prompt'],
                "duration": duration,
                "category": prompt_data['mode'],
                "success": True,
                "local_audio_path": audio_path,
                "cloud_url": cloud_url,
                "generation_time": generation_time,
                "metrics": metrics,
                "error": None,
                "human_evaluation": None
            }
            
            print(f"âœ“ æˆåŠŸ (è€—æ—¶: {generation_time:.2f}ç§’)")
            print(f"  æ–‡ä»¶å¤§å°: {metrics.get('file_size', 0)} bytes")
            print(f"  å®é™…æ—¶é•¿: {metrics.get('duration', 0):.2f}ç§’")
            
            return result
            
        except Exception as e:
            generation_time = time.time() - start_time
            error_msg = str(e)
            
            result = {
                "id": prompt_data['id'],
                "user_input": prompt_data['user_input'],
                "mode": prompt_data['mode'],
                "inspiration_chip": prompt_data['inspiration_chip'],
                "atmosphere": prompt_data['atmosphere'],
                "elements": prompt_data['elements'],
                "final_prompt": prompt_data['final_prompt'],
                "duration": 8.0,
                "category": prompt_data['mode'],
                "success": False,
                "local_audio_path": None,
                "cloud_url": None,
                "generation_time": generation_time,
                "metrics": {},
                "error": error_msg,
                "human_evaluation": None
            }
            
            print(f"âœ— å¤±è´¥: {error_msg}")
            return result
    
    async def run_batch_test(self, prompt_file: str = None, progress_file: str = None) -> List[Dict[str, Any]]:
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        # åŠ è½½prompt
        prompts = self.load_prompts(prompt_file)
        if not prompts:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„promptï¼Œé€€å‡ºæµ‹è¯•")
            return []
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯• - å…± {len(prompts)} ä¸ªprompt")
        print("æµç¨‹: åŠ è½½prompt â†’ ç”ŸæˆéŸ³é¢‘ â†’ ä¸Šä¼ äº‘å­˜å‚¨ â†’ æµ‹è¯„")
        
        start_time = time.time()
        
        results = []
        total = len(prompts)
        for i, prompt_data in enumerate(prompts, 1):
            print(f"\n[{i}/{total}] è¿›åº¦")
            result = await self.run_single_test(prompt_data)
            results.append(result)
            # å†™å…¥è¿›åº¦æ–‡ä»¶
            if progress_file:
                progress = {
                    "current": i,
                    "total": total,
                    "results": [
                        {"id": r["id"], "final_prompt": r["final_prompt"]} for r in results
                    ]
                }
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress, f, ensure_ascii=False)
        
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        print(f"\n" + "="*60)
        print(f"âœ… æµ‹è¯•å®Œæˆ!")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"æˆåŠŸ: {len(successful)}/{len(results)}")
        print(f"å¤±è´¥: {len(failed)}/{len(results)}")
        
        if successful:
            avg_time = np.mean([r['generation_time'] for r in successful])
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}ç§’")
        
        self.results = results
        return results
    
    def generate_evaluation_interface(self):
        """ç”Ÿæˆäººå·¥è¯„ä¼°ç•Œé¢"""
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœå¯è¯„ä¼°")
            return
        
        successful_results = [r for r in self.results if r['success']]
        if not successful_results:
            print("æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœå¯è¯„ä¼°")
            return
        
        # ç”ŸæˆHTMLè¯„ä¼°ç•Œé¢
        html_content = self.create_evaluation_html(successful_results)
        
        evaluation_file = self.output_dir / "evaluation" / "human_evaluation.html"
        with open(evaluation_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"\nğŸ¯ äººå·¥è¯„ä¼°ç•Œé¢å·²ç”Ÿæˆ: {evaluation_file}")
        print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ­¤æ–‡ä»¶è¿›è¡Œäººå·¥è¯„ä¼°")
        
        # å°è¯•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
        try:
            webbrowser.open(f"file://{evaluation_file.absolute()}")
        except:
            print("æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œè¯·æ‰‹åŠ¨æ‰“å¼€è¯„ä¼°æ–‡ä»¶")
    
    def create_evaluation_html(self, results: List[Dict[str, Any]]) -> str:
        """åˆ›å»ºHTMLè¯„ä¼°ç•Œé¢ - ç²¾ç®€ç‰ˆ"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Stable Audio Evaluation Interface</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}
        .test-item {{ background: white; margin: 10px 0; padding: 20px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }}
        .test-info {{ margin-bottom: 15px; }}
        .audio-player {{ margin: 10px 0; }}
        .evaluation-form {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 15px 0; }}
        .rating-group {{ display: flex; flex-direction: column; }}
        .rating-group label {{ font-weight: bold; margin-bottom: 5px; }}
        .rating-group select {{ padding: 8px; border: 1px solid #ddd; border-radius: 3px; }}
        .comments {{ width: 100%; height: 80px; padding: 8px; border: 1px solid #ddd; border-radius: 3px; resize: vertical; }}
        .submit-btn {{ background: #3498db; color: white; padding: 10px 20px; border: none; border-radius: 3px; cursor: pointer; }}
        .submit-btn:hover {{ background: #2980b9; }}
        .progress {{ background: #ecf0f1; padding: 10px; border-radius: 3px; margin: 10px 0; }}
        .url-info {{ background: #ecf0f1; padding: 10px; border-radius: 3px; margin: 10px 0; font-family: monospace; word-break: break-all; font-size: 12px; }}
        .audio-error {{ color: #e74c3c; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸµ Stable Audio Evaluation Interface</h1>
            <p>Please evaluate the quality of each audio sample. Click "Save Evaluation Results" when finished.</p>
            <p><strong>Audio Source:</strong> Supabase Cloud Storage</p>
        </div>
        
        <div class="progress">
            <span id="progress-text">Evaluation Progress: 0/{len(results)}</span>
            <div style="width: 100%; background: #ddd; height: 20px; border-radius: 10px; overflow: hidden;">
                <div id="progress-bar" style="width: 0%; background: #3498db; height: 100%; transition: width 0.3s;"></div>
            </div>
        </div>
        
        <form id="evaluation-form">
"""
        for i, result in enumerate(results):
            audio_url = result.get('cloud_url') or f"/static/generated_audio/{os.path.basename(result['local_audio_path'])}"
            html_content += f"""
            <div class="test-item" data-id="{result['id']}">
                <div class="test-info">
                    <h3>Test {i+1}: {result['id']}</h3>
                    <p><strong>Duration:</strong> {result.get('metrics', {}).get('duration', 0):.2f} seconds</p>
                    <p><strong>File Size:</strong> {result.get('metrics', {}).get('file_size', 0)} bytes</p>
                    <div class="url-info">
                        <strong>Audio URL:</strong><br>
                        {audio_url}
                    </div>
                </div>
                <div class="audio-player">
                    <p><strong>Audio Player:</strong></p>
                    <audio controls style="width: 100%;">
                        <source src="{audio_url}" type="audio/wav">
                        Your browser does not support audio playback.
                    </audio>
                    <p class="audio-error" id="audio-error-{i}" style="display: none;">
                        âš ï¸ Audio cannot be played, please check network connection or URL
                    </p>
                </div>
                <div class="evaluation-form">
                    <div class="rating-group">
                        <label for="relevance_{i}">Relevance (1-5):</label>
                        <select name="relevance_{i}" id="relevance_{i}" required>
                            <option value="">Please select</option>
                            <option value="1">1 - Completely irrelevant</option>
                            <option value="2">2 - Partially relevant</option>
                            <option value="3">3 - Moderately relevant</option>
                            <option value="4">4 - Highly relevant</option>
                            <option value="5">5 - Perfectly relevant</option>
                        </select>
                    </div>
                    <div class="rating-group">
                        <label for="quality_{i}">Audio Quality (1-5):</label>
                        <select name="quality_{i}" id="quality_{i}" required>
                            <option value="">Please select</option>
                            <option value="1">1 - Very poor quality</option>
                            <option value="2">2 - Poor quality</option>
                            <option value="3">3 - Average quality</option>
                            <option value="4">4 - Good quality</option>
                            <option value="5">5 - Excellent quality</option>
                        </select>
                    </div>
                    <div class="rating-group">
                        <label for="enjoyment_{i}">Listening Enjoyment (1-5):</label>
                        <select name="enjoyment_{i}" id="enjoyment_{i}" required>
                            <option value="">Please select</option>
                            <option value="1">1 - Not enjoyable at all</option>
                            <option value="2">2 - Slightly enjoyable</option>
                            <option value="3">3 - Moderately enjoyable</option>
                            <option value="4">4 - Quite enjoyable</option>
                            <option value="5">5 - Very enjoyable</option>
                        </select>
                    </div>
                    <div class="rating-group">
                        <label for="usability_{i}">Usability (1-5):</label>
                        <select name="usability_{i}" id="usability_{i}" required>
                            <option value="">Please select</option>
                            <option value="1">1 - Completely unusable</option>
                            <option value="2">2 - Mostly unusable</option>
                            <option value="3">3 - Barely usable</option>
                            <option value="4">4 - Quite usable</option>
                            <option value="5">5 - Highly usable</option>
                        </select>
                    </div>
                </div>
                <div>
                    <label for="comments_{i}">Detailed Comments:</label>
                    <textarea name="comments_{i}" id="comments_{i}" class="comments" placeholder="Please describe your experience and suggestions in detail..."></textarea>
                </div>
            </div>
        """
        html_content += f"""
            <div style="text-align: center; margin: 20px 0;">
                <button type="submit" class="submit-btn">ğŸ’¾ Save Evaluation Results</button>
            </div>
        </form>
    </div>
    <script>
        // Check if audio files exist
        function checkAudioFiles() {{
            const audioElements = document.querySelectorAll('audio');
            audioElements.forEach((audio, index) => {{
                audio.addEventListener('error', function() {{
                    const errorElement = document.getElementById(`audio-error-${{index}}`);
                    if (errorElement) {{
                        errorElement.style.display = 'block';
                    }}
                }});
                audio.addEventListener('loadstart', function() {{
                    const errorElement = document.getElementById(`audio-error-${{index}}`);
                    if (errorElement) {{
                        errorElement.style.display = 'none';
                    }}
                }});
            }});
        }}
        function updateProgress() {{
            const form = document.getElementById('evaluation-form');
            const selects = form.querySelectorAll('select[required]');
            const totalRequired = selects.length;
            let completed = 0;
            selects.forEach(select => {{
                if (select.value !== '') {{
                    completed++;
                }}
            }});
            const progressText = document.getElementById('progress-text');
            const progressBar = document.getElementById('progress-bar');
            const percentage = (completed / totalRequired) * 100;
            progressText.textContent = `Evaluation Progress: ${{completed}}/${{totalRequired}}`;
            progressBar.style.width = percentage + '%';
        }}
        document.querySelectorAll('select').forEach(select => {{
            select.addEventListener('change', updateProgress);
        }});
        document.getElementById('evaluation-form').addEventListener('submit', function(e) {{
            e.preventDefault();
            const formData = new FormData(this);
            const evaluationData = {{}};
            for (let i = 0; i < {len(results)}; i++) {{
                evaluationData[i] = {{
                    relevance: formData.get(`relevance_${{i}}`),
                    quality: formData.get(`quality_${{i}}`),
                    enjoyment: formData.get(`enjoyment_${{i}}`),
                    usability: formData.get(`usability_${{i}}`),
                    comments: formData.get(`comments_${{i}}`)
                }};
            }}
            const dataStr = JSON.stringify(evaluationData, null, 2);
            const dataBlob = new Blob([dataStr], {{type: 'application/json'}});
            const url = URL.createObjectURL(dataBlob);
            const link = document.createElement('a');
            link.href = url;
            link.download = 'human_evaluation_results.json';
            link.click();
            alert('Evaluation results saved!');
        }});
        updateProgress();
        checkAudioFiles();
    </script>
</body>
</html>
        """
        return html_content
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†JSONç»“æœ
        json_file = self.output_dir / "reports" / f"audio_test_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç®€åŒ–çš„CSVæŠ¥å‘Š
        csv_data = []
        for result in self.results:
            csv_data.append({
                "id": result['id'],
                "user_input": result['user_input'],
                "mode": result['mode'],
                "category": result['category'],
                "final_prompt": result['final_prompt'],
                "success": result['success'],
                "generation_time": result['generation_time'],
                "duration": result.get('metrics', {}).get('duration', 0),
                "file_size": result.get('metrics', {}).get('file_size', 0),
                "cloud_url": result.get('cloud_url', ''),
                "error": result.get('error', '')
            })
        
        df = pd.DataFrame(csv_data)
        csv_file = self.output_dir / "reports" / f"audio_test_results_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ç”ŸæˆåŸºç¡€ç»Ÿè®¡æŠ¥å‘Š
        successful = [r for r in self.results if r['success']]
        failed = [r for r in self.results if not r['success']]
        
        report = {
            "summary": {
                "total_tests": len(self.results),
                "successful": len(successful),
                "failed": len(failed),
                "success_rate": len(successful) / len(self.results) * 100 if self.results else 0
            },
            "performance": {
                "avg_generation_time": np.mean([r['generation_time'] for r in successful]) if successful else 0,
                "avg_duration": np.mean([r.get('metrics', {}).get('duration', 0) for r in successful]) if successful else 0,
                "avg_file_size": np.mean([r.get('metrics', {}).get('file_size', 0) for r in successful]) if successful else 0
            },
            "failed_tests": [
                {"id": r['id'], "user_input": r['user_input'], "error": r['error']} 
                for r in failed
            ]
        }
        
        report_file = self.output_dir / "reports" / f"audio_test_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜:")
        print(f"  è¯¦ç»†ç»“æœ: {json_file}")
        print(f"  CSVæ•°æ®: {csv_file}")
        print(f"  ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
        
        # æ‰“å°åŸºç¡€ç»Ÿè®¡
        if successful:
            print(f"\nğŸ“ˆ åŸºç¡€ç»Ÿè®¡:")
            print(f"  æˆåŠŸç‡: {len(successful)/len(self.results)*100:.1f}%")
            print(f"  å¹³å‡ç”Ÿæˆæ—¶é—´: {np.mean([r['generation_time'] for r in successful]):.2f}ç§’")
            print(f"  å¹³å‡æ—¶é•¿: {np.mean([r.get('metrics', {}).get('duration', 0) for r in successful]):.2f}ç§’")
            print(f"  å¹³å‡æ–‡ä»¶å¤§å°: {np.mean([r.get('metrics', {}).get('file_size', 0) for r in successful]):.0f} bytes")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸµ éŸ³é¢‘ç”Ÿæˆæµ‹è¯•å·¥å…·")
    print("=" * 60)
    print("è¯»å–promptæ–‡ä»¶ï¼Œç”ŸæˆéŸ³é¢‘ï¼Œåˆ›å»ºè¯„ä¼°ç•Œé¢")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = AudioTester()
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    results = await tester.run_batch_test()
    
    # ä¿å­˜ç»“æœ
    tester.save_results()
    
    # ç”Ÿæˆè¯„ä¼°ç•Œé¢
    tester.generate_evaluation_interface()
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("è¯·æ‰“å¼€ç”Ÿæˆçš„è¯„ä¼°ç•Œé¢è¿›è¡Œäººå·¥è¯„ä¼°")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--prompt', type=str, default=None, help='Promptæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default=None, help='ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, default=None, help='éŸ³é¢‘è¾“å‡ºç›®å½•')
    parser.add_argument('--progress_file', type=str, default=None, help='è¿›åº¦æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    tester = AudioTester()
    results = asyncio.run(tester.run_batch_test(prompt_file=args.prompt, progress_file=args.progress_file))
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2) 